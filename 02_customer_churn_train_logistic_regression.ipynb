{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "# Standard libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Azure ML SDK libraries\n",
    "from azureml.core import Workspace, Model, Dataset, Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.train.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need programmatic access to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.0.85 of the Azure ML SDK.\n",
      "\n",
      "Workspace name: sbazuremlws\n",
      "Azure region: westeurope\n",
      "Subscription id: bf088f59-f015-4332-bd36-54b988be7c90\n",
      "Resource group: sbazuremlrg\n"
     ]
    }
   ],
   "source": [
    "## Connect to Azure ML workspace using SDK\n",
    "# Check core SDK version number\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK.\\n\")\n",
    "\n",
    "# Load workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create / Retrieve Azure ML Training Cluster Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use this cluster for training or train locally on our Compute Instance (Notebook VM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "## Retrieve already existing training cluster or create a new one if it does not exist\n",
    "# Choose a name for the AmlCompute cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster-1\"\n",
    "\n",
    "# Check if this compute target already exists in the workspace.\n",
    "found = False\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'cpu-cluster-1':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_DS12_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                # vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "\n",
    "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy All Relevant Files to Model_Training Folder to Give Remote Experiment Run Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to copy all files that are needed for the training run to a separate folder. The remote compute will get all the files from this folder during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the experiment files (This has to be indicated as source directory in the Estimator Class below)\n",
    "training_folder = 'model_training'\n",
    "os.makedirs(training_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder \"data\"\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Azure ML dataset (authentication to underlying datastore via SAS token or access key if storage account\n",
    "# or via service principal if data lake gen 2)\n",
    "dataset_name = 'customer-churn'\n",
    "\n",
    "customerchurn = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "\n",
    "# Load the Tabular Dataset into pandas DataFrame\n",
    "telcom = customerchurn.to_pandas_dataframe()\n",
    "\n",
    "telcom.to_csv('data/customer_churn.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_training/data/customer_churn.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the data file into the experiment folder\n",
    "os.makedirs(training_folder + \"/data\", exist_ok=True)\n",
    "shutil.copy('data/customer_churn.csv', os.path.join(training_folder, \"data/customer_churn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder \"dependencies\"\n",
    "os.makedirs(\"dependencies\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dependencies/conda_dependencies.yaml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies object and store it in file\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_channel(\"plotly\")\n",
    "conda_dep.add_conda_package(\"scikit-learn\")\n",
    "conda_dep.add_conda_package(\"statsmodels\")\n",
    "conda_dep.add_conda_package(\"plotly\")\n",
    "conda_dep.add_conda_package(\"psutil\")\n",
    "conda_dep.add_conda_package(\"plotly-orca\")\n",
    "conda_dep.add_conda_package(\"matplotlib\")\n",
    "\n",
    "conda_dep.save(path='dependencies/conda_dependencies.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_training/dependencies/conda_dependencies.yaml'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the dependencies file into the experiment folder\n",
    "os.makedirs(training_folder + \"/dependencies\", exist_ok=True)\n",
    "shutil.copy(\"dependencies/conda_dependencies.yaml\", os.path.join(training_folder, \"dependencies/conda_dependencies.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Copy the utilities folder into the experiment folder\\ntry:\\n    shutil.copytree(src=\"utilities\", dst=os.path.join(training_folder, \"utilities/\"), copy_function = shutil.copy)\\nexcept FileExistsError:\\n    print(\"The utilities folder has already been copied. Please delete it first.\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the utility files into the experiment folder\n",
    "os.makedirs(training_folder + \"/utilities\", exist_ok=True)\n",
    "shutil.copy(\"utilities/preprocess_dataset.py\", os.path.join(training_folder, \"utilities/preprocess_dataset.py\"))\n",
    "\n",
    "'''# Copy the utilities folder into the experiment folder\n",
    "try:\n",
    "    shutil.copytree(src=\"utilities\", dst=os.path.join(training_folder, \"utilities/\"), copy_function = shutil.copy)\n",
    "except FileExistsError:\n",
    "    print(\"The utilities folder has already been copied. Please delete it first.\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/customer_churn_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/customer_churn_training.py\n",
    "\n",
    "### Setup\n",
    "## Import libraries\n",
    "# Import preprocessing function which is defined in utilities folder\n",
    "import utilities.preprocess_dataset\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Run\n",
    "\n",
    "# Data preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model training libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import scorer, f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Create outputs folder (to store model)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "'''## Orca configuration (this is needed for plotly graphs)\n",
    "pipe = subprocess.Popen(\"which orca\", shell=True, stdout=subprocess.PIPE).stdout\n",
    "orca_path = pipe.read()[1:-1].decode('UTF-8')\n",
    "print(\"\\n Orca Path \\n: \" + orca_path)\n",
    "\n",
    "# Path of executable\n",
    "pio.orca.config.executable = \"/\" + orca_path\n",
    "# Configure plotly.py to run orca using Xvfb\n",
    "pio.orca.config.use_xvfb = True\n",
    "# Save config\n",
    "pio.orca.config.save()'''\n",
    "\n",
    "## Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "## Expose model hyperparameters and cross validation parameters as script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate_param_1', type=float, dest='reg_rate_param_1', default=[10])\n",
    "parser.add_argument('--reg_rate_param_2', type=float, dest='reg_rate_param_2', default=[30])\n",
    "parser.add_argument('--reg_rate_param_3', type=float, dest='reg_rate_param_3', default=[100])\n",
    "parser.add_argument('--reg_rate_param_4', type=float, dest='reg_rate_param_4', default=[300])\n",
    "parser.add_argument('--reg_rate_param_5', type=float, dest='reg_rate_param_5', default=[1000])\n",
    "parser.add_argument('--cv_scorer', type=str, dest='cv_scorer', default='roc_auc')\n",
    "parser.add_argument('--cv_folds', type=int, dest='cv_folds', default=5)\n",
    "args = parser.parse_args()\n",
    "reg_rate_param_1 = args.reg_rate_param_1\n",
    "reg_rate_param_2 = args.reg_rate_param_2\n",
    "reg_rate_param_3 = args.reg_rate_param_3\n",
    "reg_rate_param_4 = args.reg_rate_param_4\n",
    "reg_rate_param_5 = args.reg_rate_param_5\n",
    "cv_scorer = args.cv_scorer\n",
    "cv_folds = args.cv_folds\n",
    "\n",
    "# Make a list that contains the different hyperparameters to be tested in cross validation\n",
    "reg_rate_params = [reg_rate_param_1, reg_rate_param_2, reg_rate_param_3, reg_rate_param_4, reg_rate_param_5]\n",
    "\n",
    "run.log('Cross Validation Scoring Metric', cv_scorer) # this is retrieved as script argument\n",
    "run.log('Count of Cross Validation Folds', cv_folds) # this is retrieved as script argument\n",
    "run.log_list('Tested Regularization Hyperparameters C', reg_rate_params) # this is retrieved as script argument\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\n Loading Data... \\n\")\n",
    "telcom = pd.read_csv('data/customer_churn.csv')\n",
    "\n",
    "telcom.drop(\"PartitionDate\", axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n Preprocessing Data... \\n\")\n",
    "# Preprocess dataset\n",
    "X, y  = utilities.preprocess_dataset.run(telcom)\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = X.nunique()[X.nunique() < 6].keys().tolist() # get columns with less than 6 unique values\n",
    "\n",
    "# Get numerical features\n",
    "numeric_features = [x for x in X.columns if x not in categorical_features]\n",
    "\n",
    "# Get all feature columns\n",
    "features = categorical_features + numeric_features\n",
    "\n",
    "# Create the preprocessing pipelines for both numeric and categorical features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now this is a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(class_weight=None, dual=False, fit_intercept=True,\n",
    "                                                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "                                                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "                                                        verbose=0, warm_start=False))])\n",
    "\n",
    "parameters={\"classifier__C\":reg_rate_params}\n",
    "\n",
    "# Split dataset into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(clf, parameters, scoring=cv_scorer, cv=cv_folds)\n",
    "\n",
    "# Fit the cv pipeline\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Get best parameter from cross validation\n",
    "best_tested_param = cv.best_params_\n",
    "print (\"Best parameter for C : \", best_tested_param)\n",
    "run.log(\"Best C parameter\", best_tested_param)    \n",
    "\n",
    "# Take the best classifier of cross validation\n",
    "clf = cv.best_estimator_\n",
    "\n",
    "# Fit the best classifier again to retrieve feature importance\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "## Model evaluation\n",
    "predictions   = clf.predict(X_test)\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "\n",
    "# Create feature importance\n",
    "features_after_onehot = numeric_features # initialize list with numeric features as first two coefficients are the numeric features\n",
    "\n",
    "# Add all one-hot-encoded categorical features in the right order\n",
    "for i in range(len(clf.named_steps['preprocessor'].named_transformers_[\"cat\"].named_steps['onehotencoder'].categories_)):\n",
    "    for j in clf.named_steps['preprocessor'].named_transformers_[\"cat\"].named_steps['onehotencoder'].categories_[i]:\n",
    "        features_after_onehot.append(categorical_features[i]+ \"_\" + j)\n",
    "        \n",
    "# Get coefficients from classifier\n",
    "coefficients  = pd.DataFrame(clf.named_steps['classifier'].coef_.ravel())\n",
    "\n",
    "# Create dataframe of  feature columns\n",
    "column_df     = pd.DataFrame(features_after_onehot)\n",
    "\n",
    "# Merge feature columns with coefficients\n",
    "coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                          right_index= True, how = \"left\"))\n",
    "\n",
    "coef_sumry.columns = [\"coefficients\", \"features\"]\n",
    "\n",
    "# Sort coefficients according to size\n",
    "coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "class_report = classification_report(y_test,predictions)\n",
    "class_metrics = precision_recall_fscore_support(y_test, predictions, beta=1.0, average=\"binary\", pos_label=0)\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "conf_matrix = confusion_matrix(y_test,predictions)\n",
    "model_roc_auc = roc_auc_score(y_test,predictions)\n",
    "\n",
    "precision = class_metrics[0]\n",
    "recall = class_metrics[1]\n",
    "fscore = class_metrics[2]\n",
    "\n",
    "# Log and print model evaluation information\n",
    "print (clf)\n",
    "\n",
    "print (\"\\n Classification report : \\n\", class_report)\n",
    "\n",
    "print (\"Accuracy Score : \", accuracy)\n",
    "run.log(\"Accuracy\", np.float(accuracy))    \n",
    "\n",
    "print (\"Area Under Curve : \",model_roc_auc,\"\\n\")\n",
    "run.log(\"AUC\", np.float(model_roc_auc))\n",
    "\n",
    "print (\"Precision : \",precision,\"\\n\")\n",
    "run.log(\"Precision\", np.float(precision))\n",
    "\n",
    "print (\"Recall : \",recall,\"\\n\")\n",
    "run.log(\"Recall\", np.float(recall))\n",
    "\n",
    "print (\"F Score : \",fscore,\"\\n\")\n",
    "run.log(\"F Score\", np.float(fscore))\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_test,probabilities[:,1])\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = [\"Churn\", \"Non Churn\"]\n",
    "\n",
    "trace1 = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                               display_labels=class_names,\n",
    "                               cmap=\"inferno\",\n",
    "                               normalize=\"true\")\n",
    "\n",
    "trace1.ax_.set_title(\"Normalized Confusion Matrix\")\n",
    "trace1.ax_.grid(False)\n",
    "run.log_image(\"Normalized Confusion Matrix\", plot=plt)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Plot roc curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % model_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "run.log_image(\"Receiver Operating Characteristic\", plot=plt)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Plot feature importance\n",
    "x = coef_sumry[\"features\"]\n",
    "y = coef_sumry[\"coefficients\"]\n",
    "\n",
    "mask1 = y >= 0\n",
    "mask2 = y < 0\n",
    "\n",
    "objects = coef_sumry[\"features\"]\n",
    "x_pos = np.arange(len(coef_sumry[\"features\"]))\n",
    "\n",
    "plt.bar(x[mask1], y[mask1], color = 'green')\n",
    "plt.bar(x[mask2], y[mask2], color = 'red')\n",
    "plt.xticks(x_pos, x, rotation='vertical')\n",
    "plt.yticks(rotation='horizontal')\n",
    "plt.ylabel('Feature Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "run.log_image(\"Feature Importance\", plot=plt)\n",
    "    \n",
    "plt.clf()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "joblib.dump(value=clf, filename='outputs/customer_churn_log_reg_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use an Estimator to Run the Script as an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator needs a source directory where it gets all files relevant for training.\n",
    "An entry script (training script) must be specified as well as the variables that have\n",
    "been parameterized in the training script (here the parameters to be tested for the C parameter of LogisticRegression).\n",
    "Also, a dependencies file with all the libraries need for model training has to be specified to build the docker image.\n",
    "The training can be done locally or with the create remote compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: customer-churn-log-reg-experiment_1585980078_7ccf0f9f\n",
      "Web View: https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585980078_7ccf0f9f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ customer_churn_training.py ] with arguments: ['--reg_rate_param_1', '100', '--reg_rate_param_2', '300', '--reg_rate_param_3', '10', '--reg_rate_param_4', '30', '--reg_rate_param_5', '1000', '--cv_scorer', 'roc_auc', '--cv_folds', '5']\n",
      "After variable expansion, calling script [ customer_churn_training.py ] with arguments: ['--reg_rate_param_1', '100', '--reg_rate_param_2', '300', '--reg_rate_param_3', '10', '--reg_rate_param_4', '30', '--reg_rate_param_5', '1000', '--cv_scorer', 'roc_auc', '--cv_folds', '5']\n",
      "\n",
      "/azureml-envs/azureml_51aec4f085a02fbe24f1bbffbfb356da/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\n",
      " Loading Data... \n",
      "\n",
      "\n",
      " Preprocessing Data... \n",
      "\n",
      "Best parameter for C :  {'classifier__C': 10.0}\n",
      "Pipeline(memory=None,\n",
      "         steps=[('preprocessor',\n",
      "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
      "                                   sparse_threshold=0.3,\n",
      "                                   transformer_weights=None,\n",
      "                                   transformers=[('num',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=[('imputer',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_value=None,\n",
      "                                                                                 missing_values=nan,\n",
      "                                                                                 strategy='median',\n",
      "                                                                                 verbose=0)),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler(copy=True,\n",
      "                                                                                  with_mean...\n",
      "                                                   'StreamingMovies',\n",
      "                                                   'Contract',\n",
      "                                                   'PaperlessBilling',\n",
      "                                                   'PaymentMethod',\n",
      "                                                   'tenure_group'])],\n",
      "                                   verbose=False)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=10.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='ovr', n_jobs=1, penalty='l2',\n",
      "                                    random_state=None, solver='liblinear',\n",
      "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      " Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      1061\n",
      "           1       0.65      0.55      0.60       348\n",
      "\n",
      "    accuracy                           0.82      1409\n",
      "   macro avg       0.76      0.73      0.74      1409\n",
      "weighted avg       0.81      0.82      0.81      1409\n",
      "\n",
      "Accuracy Score :  0.8161816891412349\n",
      "Area Under Curve :  0.7273229549221619 \n",
      "\n",
      "Precision :  0.8599640933572711 \n",
      "\n",
      "Recall :  0.9029217719132894 \n",
      "\n",
      "F Score :  0.8809195402298852 \n",
      "\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.29790782928466797 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: customer-churn-log-reg-experiment_1585980078_7ccf0f9f\n",
      "Web View: https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585980078_7ccf0f9f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'customer-churn-log-reg-experiment_1585980078_7ccf0f9f',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-04-04T06:01:22.22988Z',\n",
       " 'endTimeUtc': '2020-04-04T06:01:34.888675Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': '641166df-0264-434f-bffb-e088dc4fb155',\n",
       "  'azureml.git.repository_uri': 'https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '6ea94434c5f95819c86416341d818dcc1c92eb77',\n",
       "  'mlflow.source.git.commit': '6ea94434c5f95819c86416341d818dcc1c92eb77',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'inputDatasets': [],\n",
       " 'runDefinition': {'script': 'customer_churn_training.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--reg_rate_param_1',\n",
       "   '100',\n",
       "   '--reg_rate_param_2',\n",
       "   '300',\n",
       "   '--reg_rate_param_3',\n",
       "   '10',\n",
       "   '--reg_rate_param_4',\n",
       "   '30',\n",
       "   '--reg_rate_param_5',\n",
       "   '1000',\n",
       "   '--cv_scorer',\n",
       "   'roc_auc',\n",
       "   '--cv_folds',\n",
       "   '5'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment customer-churn-log-reg-experiment Environment',\n",
       "   'version': 'Autosave_2020-04-03T10:00:06Z_ebaccc96',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge', 'plotly'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults']},\n",
       "      'scikit-learn',\n",
       "      'statsmodels',\n",
       "      'plotly',\n",
       "      'psutil',\n",
       "      'plotly-orca',\n",
       "      'matplotlib'],\n",
       "     'name': 'azureml_51aec4f085a02fbe24f1bbffbfb356da'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=qlAntEBcyx2Pkxut6%2ByeIStUcUlJYDgbwkQKgkeUy2I%3D&st=2020-04-04T05%3A51%3A36Z&se=2020-04-04T14%3A01%3A36Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=Dnkri%2ByGf2zcdLBdV6yD4adcXoZ%2Fz97W83f7HxLOg6o%3D&st=2020-04-04T05%3A51%3A36Z&se=2020-04-04T14%3A01%3A36Z&sp=r',\n",
       "  'logs/azureml/9_azureml.log': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/logs/azureml/9_azureml.log?sv=2019-02-02&sr=b&sig=W62DsWUonMz0duMtCy5KDW%2BpPyi48hdOHw%2F4hBNuCx8%3D&st=2020-04-04T05%3A51%3A36Z&se=2020-04-04T14%3A01%3A36Z&sp=r'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_training_mode = 'local' # can be replaced by compute_target\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory=training_folder,\n",
    "                      entry_script='customer_churn_training.py',\n",
    "                      script_params = {'--reg_rate_param_1': 100,\n",
    "                                       '--reg_rate_param_2': 300,\n",
    "                                       '--reg_rate_param_3': 10,\n",
    "                                       '--reg_rate_param_4': 30,\n",
    "                                       '--reg_rate_param_5': 1000,\n",
    "                                       '--cv_scorer': \"roc_auc\",\n",
    "                                       '--cv_folds': 5},\n",
    "                      compute_target= estimator_training_mode,\n",
    "                      conda_dependencies_file = 'dependencies/conda_dependencies.yaml'\n",
    "                      )\n",
    "\n",
    "# Create an experiment\n",
    "experiment_name = 'customer-churn-log-reg-experiment'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "# Run the experiment based on the estimator\n",
    "run = experiment.submit(config=estimator)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47281a152dda4a568729a1a9c565b288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585980078_7ccf0f9f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"run_properties\": {\"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"created_utc\": \"2020-04-04T06:01:21.347744Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"641166df-0264-434f-bffb-e088dc4fb155\", \"azureml.git.repository_uri\": \"https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"6ea94434c5f95819c86416341d818dcc1c92eb77\", \"mlflow.source.git.commit\": \"6ea94434c5f95819c86416341d818dcc1c92eb77\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-04-04T06:01:34.888675Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=IcyYA6PWgCMBhJNOTzpxoNYnJsJKucXmN5%2F6Q4zLMAc%3D&st=2020-04-04T05%3A51%3A37Z&se=2020-04-04T14%3A01%3A37Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=mM8habGyestXl4b%2FYAh3MCixHjSSf2g9Sv%2FRi3CYucs%3D&st=2020-04-04T05%3A51%3A37Z&se=2020-04-04T14%3A01%3A37Z&sp=r\", \"logs/azureml/9_azureml.log\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/logs/azureml/9_azureml.log?sv=2019-02-02&sr=b&sig=1eWNQCFPY1NKbYcTRDtW1xkYSGhsid%2Brp4Ys8VAKy48%3D&st=2020-04-04T05%3A51%3A37Z&se=2020-04-04T14%3A01%3A37Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/9_azureml.log\"], [\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"]], \"run_duration\": \"0:00:13\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"Cross Validation Scoring Metric\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [\"roc_auc\"]}]}, {\"name\": \"Count of Cross Validation Folds\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [5]}]}, {\"name\": \"Tested Regularization Hyperparameters C\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0, 1, 2, 3, 4], \"series\": [{\"data\": [100.0, 300, 10, 30, 1000]}]}, {\"name\": \"Best C parameter\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [\"{'classifier__C': 10.0}\"]}]}, {\"name\": \"Accuracy\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [0.8161816891412349]}]}, {\"name\": \"AUC\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [0.7273229549221619]}]}, {\"name\": \"Precision\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [0.8599640933572711]}]}, {\"name\": \"Recall\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [0.9029217719132894]}]}, {\"name\": \"F Score\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [0.8809195402298852]}]}, {\"name\": \"Normalized Confusion Matrix\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/Normalized Confusion Matrix_1585980090.png\"]}]}, {\"name\": \"Receiver Operating Characteristic\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/Receiver Operating Characteristic_1585980090.png\"]}]}, {\"name\": \"Feature Importance\", \"run_id\": \"customer-churn-log-reg-experiment_1585980078_7ccf0f9f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585980078_7ccf0f9f/Feature Importance_1585980091.png\"]}]}], \"run_logs\": \"Starting the daemon thread to refresh tokens in background for process with pid = 9\\nEntering Run History Context Manager.\\nPreparing to call script [ customer_churn_training.py ] with arguments: ['--reg_rate_param_1', '100', '--reg_rate_param_2', '300', '--reg_rate_param_3', '10', '--reg_rate_param_4', '30', '--reg_rate_param_5', '1000', '--cv_scorer', 'roc_auc', '--cv_folds', '5']\\nAfter variable expansion, calling script [ customer_churn_training.py ] with arguments: ['--reg_rate_param_1', '100', '--reg_rate_param_2', '300', '--reg_rate_param_3', '10', '--reg_rate_param_4', '30', '--reg_rate_param_5', '1000', '--cv_scorer', 'roc_auc', '--cv_folds', '5']\\n\\n/azureml-envs/azureml_51aec4f085a02fbe24f1bbffbfb356da/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\\n  warnings.warn(message, FutureWarning)\\n\\n Loading Data... \\n\\n\\n Preprocessing Data... \\n\\nBest parameter for C :  {'classifier__C': 10.0}\\nPipeline(memory=None,\\n         steps=[('preprocessor',\\n                 ColumnTransformer(n_jobs=None, remainder='drop',\\n                                   sparse_threshold=0.3,\\n                                   transformer_weights=None,\\n                                   transformers=[('num',\\n                                                  Pipeline(memory=None,\\n                                                           steps=[('imputer',\\n                                                                   SimpleImputer(add_indicator=False,\\n                                                                                 copy=True,\\n                                                                                 fill_value=None,\\n                                                                                 missing_values=nan,\\n                                                                                 strategy='median',\\n                                                                                 verbose=0)),\\n                                                                  ('scaler',\\n                                                                   StandardScaler(copy=True,\\n                                                                                  with_mean...\\n                                                   'StreamingMovies',\\n                                                   'Contract',\\n                                                   'PaperlessBilling',\\n                                                   'PaymentMethod',\\n                                                   'tenure_group'])],\\n                                   verbose=False)),\\n                ('classifier',\\n                 LogisticRegression(C=10.0, class_weight=None, dual=False,\\n                                    fit_intercept=True, intercept_scaling=1,\\n                                    l1_ratio=None, max_iter=100,\\n                                    multi_class='ovr', n_jobs=1, penalty='l2',\\n                                    random_state=None, solver='liblinear',\\n                                    tol=0.0001, verbose=0, warm_start=False))],\\n         verbose=False)\\n\\n Classification report : \\n               precision    recall  f1-score   support\\n\\n           0       0.86      0.90      0.88      1061\\n           1       0.65      0.55      0.60       348\\n\\n    accuracy                           0.82      1409\\n   macro avg       0.76      0.73      0.74      1409\\nweighted avg       0.81      0.82      0.81      1409\\n\\nAccuracy Score :  0.8161816891412349\\nArea Under Curve :  0.7273229549221619 \\n\\nPrecision :  0.8599640933572711 \\n\\nRecall :  0.9029217719132894 \\n\\nF Score :  0.8809195402298852 \\n\\n\\n\\nThe experiment completed successfully. Finalizing run...\\nLogging experiment finalizing status in history service.\\nStarting the daemon thread to refresh tokens in background for process with pid = 9\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\n2 items cleaning up...\\nCleanup took 0.29790782928466797 seconds\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_churn_log_reg_model version: 5\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7273229549221619\n",
      "\t Accuracy : 0.8161816891412349\n",
      "\t Precision : 0.8599640933572711\n",
      "\t Recall : 0.9029217719132894\n",
      "\t F Score : 0.8809195402298852\n",
      "\n",
      "\n",
      "customer_churn_log_reg_model version: 4\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7032768315318266\n",
      "\t Accuracy : 0.7998580553584103\n",
      "\t Precision : 0.8504923903312444\n",
      "\t Recall : 0.892018779342723\n",
      "\t F Score : 0.8707607699358386\n",
      "\n",
      "\n",
      "AutoML0b6c959a634 version: 1\n",
      "\n",
      "\n",
      "customer_churn_dt_model version: 1\n",
      "\t Training Context : Notebook\n",
      "\t Algorithm : Decision Tree\n",
      "\n",
      "\n",
      "customer_churn_log_reg_model version: 3\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7128652597402597\n",
      "\t Accuracy : 0.8005677785663591\n",
      "\t Precision : 0.8337825696316262\n",
      "\t Recall : 0.90625\n",
      "\t F Score : 0.8685072531586336\n",
      "\n",
      "\n",
      "customer_churn_log_reg_model version: 2\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7249834857723577\n",
      "\t Accuracy : 0.8048261178140526\n",
      "\t Precision : 0.8421532846715328\n",
      "\t Recall : 0.9004878048780488\n",
      "\t F Score : 0.8703441772748703\n",
      "\n",
      "\n",
      "customer_churn_log_reg_model version: 1\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7015653003173351\n",
      "\t Accuracy : 0.8097941802696949\n",
      "\t Precision : 0.8482999128160419\n",
      "\t Recall : 0.9119025304592315\n",
      "\t F Score : 0.8789521228545619\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the model\n",
    "run.register_model(model_path='outputs/customer_churn_log_reg_model.pkl', model_name='customer_churn_log_reg_model',\n",
    "                   tags={'Training Context':'Estimator', 'Algorithm':'Logistic Regression'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy'],\n",
    "                              'Precision': run.get_metrics()['Precision'], 'Recall': run.get_metrics()['Recall'],\n",
    "                              'F Score': run.get_metrics()['F Score']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile $training_folder/customer_churn_training_old.py\n",
    "\n",
    "### Setup\n",
    "## Import libraries\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Run\n",
    "\n",
    "# Data preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model training libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import scorer, f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "'''## Orca configuration\n",
    "pipe = subprocess.Popen(\"which orca\", shell=True, stdout=subprocess.PIPE).stdout\n",
    "orca_path = pipe.read()[1:-1].decode('UTF-8')\n",
    "print(\"\\n Orca Path \\n: \" + orca_path)\n",
    "\n",
    "# Path of executable\n",
    "pio.orca.config.executable = \"/\" + orca_path\n",
    "# Configure plotly.py to run orca using Xvfb\n",
    "pio.orca.config.use_xvfb = True\n",
    "# Save config\n",
    "pio.orca.config.save()'''\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\n Loading Data... \\n\")\n",
    "telcom = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Preprocess dataset\n",
    "telcom = preprocess_dataset(telcom)\n",
    "\n",
    "# Scaling of numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(telcom[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "\n",
    "# Dropping original values and merging scaled values for numerical columns\n",
    "telcom = telcom.drop(columns = num_cols,axis = 1)\n",
    "telcom = telcom.merge(scaled,left_index=True,right_index=True,how = \"left\")\n",
    "\n",
    "\n",
    "### Model Training\n",
    "# Split data into train and test\n",
    "train,test = train_test_split(telcom, test_size = .25 ,random_state = 42)\n",
    "    \n",
    "# Separate dependent and independent variables and exclude ID column\n",
    "cols    = [i for i in telcom.columns if i not in Id_col + target_col]\n",
    "train_X = train[cols]\n",
    "train_Y = train[target_col]\n",
    "test_X  = test[cols]\n",
    "test_Y  = test[target_col]\n",
    "\n",
    "# Write a function for generic model training    \n",
    "def telecom_churn_prediction(algorithm,training_x,testing_x,\n",
    "                             training_y,testing_y,cols,cf):\n",
    "    '''\n",
    "    Function signature\n",
    "    algorithm     - algorithm used \n",
    "    training_x    - predictor variables dataframe (training)\n",
    "    testing_x     - predictor variables dataframe (testing)\n",
    "    training_y    - target variable (training)\n",
    "    training_y    - target variable (testing)\n",
    "    cols          - features\n",
    "    cf - [\"coefficients\",\"features\"] (cooefficients for logistic regression, features for tree based models)\n",
    "    '''\n",
    "    \n",
    "    # Fit the model\n",
    "    algorithm.fit(training_x,training_y)\n",
    "    predictions   = algorithm.predict(testing_x)\n",
    "    probabilities = algorithm.predict_proba(testing_x)\n",
    "    \n",
    "    # Store coefficients\n",
    "    if   cf == \"coefficients\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n",
    "    elif cf == \"features\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n",
    "        \n",
    "    column_df     = pd.DataFrame(cols)\n",
    "    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                              right_index= True, how = \"left\"))\n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    class_report = classification_report(testing_y,predictions)\n",
    "    class_metrics = precision_recall_fscore_support(testing_y, predictions, beta=1.0, average=\"binary\", pos_label=0)\n",
    "    accuracy = accuracy_score(testing_y,predictions)\n",
    "    conf_matrix = confusion_matrix(testing_y,predictions)\n",
    "    model_roc_auc = roc_auc_score(testing_y,predictions)\n",
    "    \n",
    "    precision = class_metrics[0]\n",
    "    recall = class_metrics[1]\n",
    "    fscore = class_metrics[2]\n",
    "    \n",
    "    # Log and print model evaluation information\n",
    "    print (algorithm)\n",
    "    \n",
    "    print (\"\\n Classification report : \\n\", class_report)\n",
    "    \n",
    "    print (\"Accuracy Score : \", accuracy)\n",
    "    run.log(\"Accuracy\", np.float(accuracy))    \n",
    "    \n",
    "    print (\"Area Under Curve : \",model_roc_auc,\"\\n\")\n",
    "    run.log(\"AUC\", np.float(model_roc_auc))\n",
    "    \n",
    "    print (\"Precision : \",precision,\"\\n\")\n",
    "    run.log(\"Precision\", np.float(precision))\n",
    "    \n",
    "    print (\"Recall : \",recall,\"\\n\")\n",
    "    run.log(\"Recall\", np.float(recall))\n",
    "    \n",
    "    print (\"F Score : \",fscore,\"\\n\")\n",
    "    run.log(\"F Score\", np.float(fscore))\n",
    "    \n",
    "    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    class_names = [\"Churn\", \"Non Churn\"]\n",
    "    \n",
    "    trace1 = plot_confusion_matrix(algorithm, testing_x, testing_y,\n",
    "                                   display_labels=class_names,\n",
    "                                   cmap=\"inferno\",\n",
    "                                   normalize=\"true\")\n",
    "    \n",
    "    trace1.ax_.set_title(\"Normalized Confusion Matrix\")\n",
    "    trace1.ax_.grid(False)\n",
    "    run.log_image(\"Normalized Confusion Matrix\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot roc curve\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % model_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    run.log_image(\"Receiver Operating Characteristic\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    x = coef_sumry[\"features\"]\n",
    "    y = coef_sumry[\"coefficients\"]\n",
    "\n",
    "    mask1 = y >= 0\n",
    "    mask2 = y < 0\n",
    "\n",
    "    objects = coef_sumry[\"features\"]\n",
    "    x_pos = np.arange(len(coef_sumry[\"features\"]))\n",
    "\n",
    "    plt.bar(x[mask1], y[mask1], color = 'green')\n",
    "    plt.bar(x[mask2], y[mask2], color = 'red')\n",
    "    plt.xticks(x_pos, x, rotation='vertical')\n",
    "    plt.yticks(rotation='horizontal')\n",
    "    plt.ylabel('Feature Coefficient')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    run.log_image(\"Feature Importance\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    '''# Plot confusion matrix\n",
    "    trace1 = go.Heatmap(z = conf_matrix ,\n",
    "                        x = [\"Not churn\",\"Churn\"],\n",
    "                        y = [\"Not churn\",\"Churn\"],\n",
    "                        showscale  = False,colorscale = \"Picnic\",\n",
    "                        name = \"matrix\")\n",
    "    \n",
    "    # Plot roc curve\n",
    "    trace2 = go.Scatter(x = fpr,y = tpr,\n",
    "                        name = \"Roc : \" + str(model_roc_auc),\n",
    "                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n",
    "    trace3 = go.Scatter(x = [0,1],y=[0,1],\n",
    "                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n",
    "                        dash = 'dot'))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n",
    "                    name = \"coefficients\",\n",
    "                    marker = dict(color = coef_sumry[\"coefficients\"],\n",
    "                                  colorscale = \"Picnic\",\n",
    "                                  line = dict(width = .6,color = \"black\")))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = sp.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver operating characteristic',\n",
    "                                            'Feature Importances'))\n",
    "    \n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,2)\n",
    "    fig.append_trace(trace3,1,2)\n",
    "    fig.append_trace(trace4,2,1)\n",
    "    \n",
    "    # Plot layout\n",
    "    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n",
    "                         autosize = False,height = 900,width = 800,\n",
    "                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         margin = dict(b = 195))\n",
    "    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n",
    "    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n",
    "    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n",
    "                                        tickangle = 90))\n",
    "    \n",
    "    image_path = \"outputs/log_reg_graphs.png\"\n",
    "    fig.write_image(image_path)\n",
    "    \n",
    "    # Upload the file explicitly into artifacts \n",
    "    run.upload_file(name = image_path, path_or_stream = image_path)'''\n",
    "    \n",
    "    return algorithm\n",
    "        \n",
    "# Build logistic regression model\n",
    "\n",
    "# Expose regularization hyperparameter as script argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "run.log('Regularization Hyperparameter Lambda', np.float(reg)) # this is retrieved as script argument\n",
    "\n",
    "logit = LogisticRegression(C=1/reg, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "logit_fitted = telecom_churn_prediction(logit,train_X,test_X,train_Y,test_Y,\n",
    "                         cols,\"coefficients\")\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "joblib.dump(value=logit_fitted, filename='outputs/customer_churn_log_reg_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
