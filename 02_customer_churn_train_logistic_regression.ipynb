{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Workspace, Model, Dataset\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.0.85 of the Azure ML SDK.\n",
      "\n",
      "Workspace name: sbazuremlws\n",
      "Azure region: westeurope\n",
      "Subscription id: bf088f59-f015-4332-bd36-54b988be7c90\n",
      "Resource group: sbazuremlrg\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK.\\n\")\n",
    "\n",
    "# Load workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create / Retrieve Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for the AmlCompute cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster-1\"\n",
    "\n",
    "# Check if this compute target already exists in the workspace.\n",
    "found = False\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'cpu-cluster-1':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_DS12_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                # vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "\n",
    "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Files to Model_Training Folder for Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the experiment files\n",
    "training_folder = 'model_training'\n",
    "os.makedirs(training_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder \"data\"\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Azure ML dataset (authentication to underlying datastore via SAS token or access key)\n",
    "dataset_name = 'customer-churn'\n",
    "\n",
    "customerchurn = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "\n",
    "# Load the Tabular Dataset into pandas DataFrame\n",
    "telcom = customerchurn.to_pandas_dataframe()\n",
    "\n",
    "telcom.to_csv('data/customer_churn.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_training/customer_churn.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('data/customer_churn.csv', os.path.join(training_folder, \"customer_churn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder \"dependencies\"\n",
    "os.makedirs(\"dependencies\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dependencies/conda_dependencies.yaml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies object and store it in file\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_channel(\"plotly\")\n",
    "conda_dep.add_conda_package(\"scikit-learn\")\n",
    "conda_dep.add_conda_package(\"statsmodels\")\n",
    "conda_dep.add_conda_package(\"plotly\")\n",
    "conda_dep.add_conda_package(\"psutil\")\n",
    "conda_dep.add_conda_package(\"plotly-orca\")\n",
    "conda_dep.add_conda_package(\"matplotlib\")\n",
    "\n",
    "conda_dep.save(path='dependencies/conda_dependencies.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_training/conda_dependencies.yaml'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the dependencies file into the experiment folder\n",
    "shutil.copy('dependencies/conda_dependencies.yaml', os.path.join(training_folder, \"conda_dependencies.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder \"utilities\"\n",
    "os.makedirs(\"utilities\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The utilities folder has already been copied. Please delete it first.\n"
     ]
    }
   ],
   "source": [
    "# Copy the dependencies file into the experiment folder\n",
    "try:\n",
    "    shutil.copytree(src=\"utilities\", dst=os.path.join(training_folder, \"utilities/\"), copy_function = shutil.copy)\n",
    "except FileExistsError:\n",
    "    print(\"The utilities folder has already been copied. Please delete it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/customer_churn_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/customer_churn_training.py\n",
    "\n",
    "### Setup\n",
    "## Import libraries\n",
    "# Import preprocessing function which is defined in utilities folder\n",
    "import utilities.preprocess_dataset\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Run\n",
    "\n",
    "# Data preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model training libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import scorer, f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "'''## Orca configuration\n",
    "pipe = subprocess.Popen(\"which orca\", shell=True, stdout=subprocess.PIPE).stdout\n",
    "orca_path = pipe.read()[1:-1].decode('UTF-8')\n",
    "print(\"\\n Orca Path \\n: \" + orca_path)\n",
    "\n",
    "# Path of executable\n",
    "pio.orca.config.executable = \"/\" + orca_path\n",
    "# Configure plotly.py to run orca using Xvfb\n",
    "pio.orca.config.use_xvfb = True\n",
    "# Save config\n",
    "pio.orca.config.save()'''\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Expose regularization hyperparameter as script argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "run.log('Regularization Hyperparameter Lambda', np.float(reg)) # this is retrieved as script argument\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\n Loading Data... \\n\")\n",
    "telcom = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "print(\"\\n Preprocessing Data... \\n\")\n",
    "# Preprocess dataset\n",
    "X, y  = utilities.preprocess_dataset.run_preprocessing(telcom)\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = X.nunique()[X.nunique() < 6].keys().tolist() # get columns with less than 6 unique values\n",
    "\n",
    "# Get numerical features\n",
    "numeric_features = [x for x in X.columns if x not in categorical_features]\n",
    "\n",
    "# Get all feature columns\n",
    "features = categorical_features + numeric_features\n",
    "\n",
    "# Create the preprocessing pipelines for both numeric and categorical features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now this is a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(C=1/reg, class_weight=None, dual=False, fit_intercept=True,\n",
    "                                                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "                                                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "                                                        verbose=0, warm_start=False))])\n",
    "\n",
    "# Split dataset into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit the classifier pipeline\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions   = clf.predict(X_test)\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "\n",
    "coefficients  = pd.DataFrame(clf.named_steps['classifier'].coef_.ravel())\n",
    "    \n",
    "column_df     = pd.DataFrame(features)\n",
    "coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                          right_index= True, how = \"left\"))\n",
    "coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "\n",
    "# Evaluate the model\n",
    "class_report = classification_report(y_test,predictions)\n",
    "class_metrics = precision_recall_fscore_support(y_test, predictions, beta=1.0, average=\"binary\", pos_label=0)\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "conf_matrix = confusion_matrix(y_test,predictions)\n",
    "model_roc_auc = roc_auc_score(y_test,predictions)\n",
    "\n",
    "precision = class_metrics[0]\n",
    "recall = class_metrics[1]\n",
    "fscore = class_metrics[2]\n",
    "\n",
    "# Log and print model evaluation information\n",
    "print (clf)\n",
    "\n",
    "print (\"\\n Classification report : \\n\", class_report)\n",
    "\n",
    "print (\"Accuracy Score : \", accuracy)\n",
    "run.log(\"Accuracy\", np.float(accuracy))    \n",
    "\n",
    "print (\"Area Under Curve : \",model_roc_auc,\"\\n\")\n",
    "run.log(\"AUC\", np.float(model_roc_auc))\n",
    "\n",
    "print (\"Precision : \",precision,\"\\n\")\n",
    "run.log(\"Precision\", np.float(precision))\n",
    "\n",
    "print (\"Recall : \",recall,\"\\n\")\n",
    "run.log(\"Recall\", np.float(recall))\n",
    "\n",
    "print (\"F Score : \",fscore,\"\\n\")\n",
    "run.log(\"F Score\", np.float(fscore))\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_test,probabilities[:,1])\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = [\"Churn\", \"Non Churn\"]\n",
    "\n",
    "trace1 = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                               display_labels=class_names,\n",
    "                               cmap=\"inferno\",\n",
    "                               normalize=\"true\")\n",
    "\n",
    "trace1.ax_.set_title(\"Normalized Confusion Matrix\")\n",
    "trace1.ax_.grid(False)\n",
    "run.log_image(\"Normalized Confusion Matrix\", plot=plt)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Plot roc curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % model_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "run.log_image(\"Receiver Operating Characteristic\", plot=plt)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Plot feature importance\n",
    "x = coef_sumry[\"features\"]\n",
    "y = coef_sumry[\"coefficients\"]\n",
    "\n",
    "mask1 = y >= 0\n",
    "mask2 = y < 0\n",
    "\n",
    "objects = coef_sumry[\"features\"]\n",
    "x_pos = np.arange(len(coef_sumry[\"features\"]))\n",
    "\n",
    "plt.bar(x[mask1], y[mask1], color = 'green')\n",
    "plt.bar(x[mask2], y[mask2], color = 'red')\n",
    "plt.xticks(x_pos, x, rotation='vertical')\n",
    "plt.yticks(rotation='horizontal')\n",
    "plt.ylabel('Feature Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "run.log_image(\"Feature Importance\", plot=plt)\n",
    "    \n",
    "plt.clf()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "joblib.dump(value=clf, filename='outputs/customer_churn_log_reg_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use an Estimator to Run the Script as an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: customer-churn-log-reg-experiment_1585910122_ecd04e2f\n",
      "Web View: https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585910122_ecd04e2f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ customer_churn_training.py ] with arguments: ['--reg_rate', '0.01']\n",
      "After variable expansion, calling script [ customer_churn_training.py ] with arguments: ['--reg_rate', '0.01']\n",
      "\n",
      "/azureml-envs/azureml_51aec4f085a02fbe24f1bbffbfb356da/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\n",
      " Loading Data... \n",
      "\n",
      "\n",
      " Preprocessing Data... \n",
      "\n",
      "Pipeline(memory=None,\n",
      "         steps=[('preprocessor',\n",
      "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
      "                                   sparse_threshold=0.3,\n",
      "                                   transformer_weights=None,\n",
      "                                   transformers=[('num',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=[('imputer',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_value=None,\n",
      "                                                                                 missing_values=nan,\n",
      "                                                                                 strategy='mean',\n",
      "                                                                                 verbose=0)),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler(copy=True,\n",
      "                                                                                  with_mean=T...\n",
      "                                                   'tenure_group_Tenure_24-48',\n",
      "                                                   'tenure_group_Tenure_48-60',\n",
      "                                                   'tenure_group_Tenure_gt_60'])],\n",
      "                                   verbose=False)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=100.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='ovr', n_jobs=1, penalty='l2',\n",
      "                                    random_state=None, solver='liblinear',\n",
      "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      " Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87      1025\n",
      "           1       0.67      0.55      0.61       384\n",
      "\n",
      "    accuracy                           0.80      1409\n",
      "   macro avg       0.76      0.72      0.74      1409\n",
      "weighted avg       0.80      0.80      0.80      1409\n",
      "\n",
      "Accuracy Score :  0.8048261178140526\n",
      "Area Under Curve :  0.7249834857723577 \n",
      "\n",
      "Precision :  0.8421532846715328 \n",
      "\n",
      "Recall :  0.9004878048780488 \n",
      "\n",
      "F Score :  0.8703441772748703 \n",
      "\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.1756584644317627 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: customer-churn-log-reg-experiment_1585910122_ecd04e2f\n",
      "Web View: https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585910122_ecd04e2f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'customer-churn-log-reg-experiment_1585910122_ecd04e2f',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-04-03T10:35:25.68525Z',\n",
       " 'endTimeUtc': '2020-04-03T10:35:35.214285Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': 'ea628ccd-f638-4633-8906-a7399d46ca84',\n",
       "  'azureml.git.repository_uri': 'https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '6ea94434c5f95819c86416341d818dcc1c92eb77',\n",
       "  'mlflow.source.git.commit': '6ea94434c5f95819c86416341d818dcc1c92eb77',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'inputDatasets': [],\n",
       " 'runDefinition': {'script': 'customer_churn_training.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--reg_rate', '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment customer-churn-log-reg-experiment Environment',\n",
       "   'version': 'Autosave_2020-04-03T10:00:06Z_ebaccc96',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge', 'plotly'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults']},\n",
       "      'scikit-learn',\n",
       "      'statsmodels',\n",
       "      'plotly',\n",
       "      'psutil',\n",
       "      'plotly-orca',\n",
       "      'matplotlib'],\n",
       "     'name': 'azureml_51aec4f085a02fbe24f1bbffbfb356da'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=nnwFnD6uNunfBv6h6WEcz90fmLqs9cBuxfIWP5AvoM8%3D&st=2020-04-03T10%3A25%3A36Z&se=2020-04-03T18%3A35%3A36Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=tvgl6occ08sefiTBk2wsN99zz2nHdXZOYag7hp4%2Fsm0%3D&st=2020-04-03T10%3A25%3A36Z&se=2020-04-03T18%3A35%3A36Z&sp=r',\n",
       "  'logs/azureml/8_azureml.log': 'https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/logs/azureml/8_azureml.log?sv=2019-02-02&sr=b&sig=78HzmCLbiIAWMm0Ubs7aB6FQzLRMxhByZtjbuTdf8t4%3D&st=2020-04-03T10%3A25%3A36Z&se=2020-04-03T18%3A35%3A36Z&sp=r'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory=training_folder,\n",
    "                      entry_script='customer_churn_training.py',\n",
    "                      script_params = {'--reg_rate': 0.01},\n",
    "                      compute_target='local', # can be compute_target\n",
    "                      conda_dependencies_file = 'conda_dependencies.yaml'\n",
    "                      )\n",
    "\n",
    "# Create an experiment\n",
    "experiment_name = 'customer-churn-log-reg-experiment'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "# Run the experiment based on the estimator\n",
    "run = experiment.submit(config=estimator)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d75ee53b500413ca539b3da394cf85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/customer-churn-log-reg-experiment/runs/customer-churn-log-reg-experiment_1585910122_ecd04e2f?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/sbazuremlrg/workspaces/sbazuremlws\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"run_properties\": {\"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"created_utc\": \"2020-04-03T10:35:24.789412Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"ea628ccd-f638-4633-8906-a7399d46ca84\", \"azureml.git.repository_uri\": \"https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/sebastianbirk/customer-churn-prediction-azure-ml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"6ea94434c5f95819c86416341d818dcc1c92eb77\", \"mlflow.source.git.commit\": \"6ea94434c5f95819c86416341d818dcc1c92eb77\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-04-03T10:35:35.214285Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=cnj5bc4MdfO5%2F0kLjBIa%2FAof4ere4yxQ%2B7MWb7GMye4%3D&st=2020-04-03T10%3A25%3A39Z&se=2020-04-03T18%3A35%3A39Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=6eR1MjisT2Fvh421bPqPvj0iyd82U7Gh3eXk%2F6LJbII%3D&st=2020-04-03T10%3A25%3A39Z&se=2020-04-03T18%3A35%3A39Z&sp=r\", \"logs/azureml/8_azureml.log\": \"https://sbazuremlws5679598588.blob.core.windows.net/azureml/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/logs/azureml/8_azureml.log?sv=2019-02-02&sr=b&sig=melMGhT9Uh86mtlk4UdtH4zFurx6kRZrm4d45pJVtak%3D&st=2020-04-03T10%3A25%3A39Z&se=2020-04-03T18%3A35%3A39Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/8_azureml.log\"], [\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"]], \"run_duration\": \"0:00:10\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"Regularization Hyperparameter Lambda\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.01]}]}, {\"name\": \"Accuracy\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.8048261178140526]}]}, {\"name\": \"AUC\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.7249834857723577]}]}, {\"name\": \"Precision\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.8421532846715328]}]}, {\"name\": \"Recall\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.9004878048780488]}]}, {\"name\": \"F Score\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [0.8703441772748703]}]}, {\"name\": \"Normalized Confusion Matrix\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/Normalized Confusion Matrix_1585910130.png\"]}]}, {\"name\": \"Receiver Operating Characteristic\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/Receiver Operating Characteristic_1585910130.png\"]}]}, {\"name\": \"Feature Importance\", \"run_id\": \"customer-churn-log-reg-experiment_1585910122_ecd04e2f\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.customer-churn-log-reg-experiment_1585910122_ecd04e2f/Feature Importance_1585910131.png\"]}]}], \"run_logs\": \"Starting the daemon thread to refresh tokens in background for process with pid = 8\\nEntering Run History Context Manager.\\nPreparing to call script [ customer_churn_training.py ] with arguments: ['--reg_rate', '0.01']\\nAfter variable expansion, calling script [ customer_churn_training.py ] with arguments: ['--reg_rate', '0.01']\\n\\n/azureml-envs/azureml_51aec4f085a02fbe24f1bbffbfb356da/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\\n  warnings.warn(message, FutureWarning)\\n\\n Loading Data... \\n\\n\\n Preprocessing Data... \\n\\nPipeline(memory=None,\\n         steps=[('preprocessor',\\n                 ColumnTransformer(n_jobs=None, remainder='drop',\\n                                   sparse_threshold=0.3,\\n                                   transformer_weights=None,\\n                                   transformers=[('num',\\n                                                  Pipeline(memory=None,\\n                                                           steps=[('imputer',\\n                                                                   SimpleImputer(add_indicator=False,\\n                                                                                 copy=True,\\n                                                                                 fill_value=None,\\n                                                                                 missing_values=nan,\\n                                                                                 strategy='mean',\\n                                                                                 verbose=0)),\\n                                                                  ('scaler',\\n                                                                   StandardScaler(copy=True,\\n                                                                                  with_mean=T...\\n                                                   'tenure_group_Tenure_24-48',\\n                                                   'tenure_group_Tenure_48-60',\\n                                                   'tenure_group_Tenure_gt_60'])],\\n                                   verbose=False)),\\n                ('classifier',\\n                 LogisticRegression(C=100.0, class_weight=None, dual=False,\\n                                    fit_intercept=True, intercept_scaling=1,\\n                                    l1_ratio=None, max_iter=100,\\n                                    multi_class='ovr', n_jobs=1, penalty='l2',\\n                                    random_state=None, solver='liblinear',\\n                                    tol=0.0001, verbose=0, warm_start=False))],\\n         verbose=False)\\n\\n Classification report : \\n               precision    recall  f1-score   support\\n\\n           0       0.84      0.90      0.87      1025\\n           1       0.67      0.55      0.61       384\\n\\n    accuracy                           0.80      1409\\n   macro avg       0.76      0.72      0.74      1409\\nweighted avg       0.80      0.80      0.80      1409\\n\\nAccuracy Score :  0.8048261178140526\\nArea Under Curve :  0.7249834857723577 \\n\\nPrecision :  0.8421532846715328 \\n\\nRecall :  0.9004878048780488 \\n\\nF Score :  0.8703441772748703 \\n\\n\\n\\nThe experiment completed successfully. Finalizing run...\\nLogging experiment finalizing status in history service.\\nStarting the daemon thread to refresh tokens in background for process with pid = 8\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\n2 items cleaning up...\\nCleanup took 0.1756584644317627 seconds\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_churn_log_reg_model version: 2\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7249834857723577\n",
      "\t Accuracy : 0.8048261178140526\n",
      "\t Precision : 0.8421532846715328\n",
      "\t Recall : 0.9004878048780488\n",
      "\t F Score : 0.8703441772748703\n",
      "\n",
      "\n",
      "customer_churn_log_reg_model version: 1\n",
      "\t Training Context : Estimator\n",
      "\t Algorithm : Logistic Regression\n",
      "\t AUC : 0.7015653003173351\n",
      "\t Accuracy : 0.8097941802696949\n",
      "\t Precision : 0.8482999128160419\n",
      "\t Recall : 0.9119025304592315\n",
      "\t F Score : 0.8789521228545619\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the model\n",
    "run.register_model(model_path='outputs/customer_churn_log_reg_model.pkl', model_name='customer_churn_log_reg_model',\n",
    "                   tags={'Training Context':'Estimator', 'Algorithm':'Logistic Regression'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy'],\n",
    "                              'Precision': run.get_metrics()['Precision'], 'Recall': run.get_metrics()['Recall'],\n",
    "                              'F Score': run.get_metrics()['F Score']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile $training_folder/customer_churn_training_old.py\n",
    "\n",
    "### Setup\n",
    "## Import libraries\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Azure ML libraries\n",
    "from azureml.core import Run\n",
    "\n",
    "# Data preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model training libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import scorer, f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "'''## Orca configuration\n",
    "pipe = subprocess.Popen(\"which orca\", shell=True, stdout=subprocess.PIPE).stdout\n",
    "orca_path = pipe.read()[1:-1].decode('UTF-8')\n",
    "print(\"\\n Orca Path \\n: \" + orca_path)\n",
    "\n",
    "# Path of executable\n",
    "pio.orca.config.executable = \"/\" + orca_path\n",
    "# Configure plotly.py to run orca using Xvfb\n",
    "pio.orca.config.use_xvfb = True\n",
    "# Save config\n",
    "pio.orca.config.save()'''\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\n Loading Data... \\n\")\n",
    "telcom = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Preprocess dataset\n",
    "telcom = preprocess_dataset(telcom)\n",
    "\n",
    "# Scaling of numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(telcom[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "\n",
    "# Dropping original values and merging scaled values for numerical columns\n",
    "telcom = telcom.drop(columns = num_cols,axis = 1)\n",
    "telcom = telcom.merge(scaled,left_index=True,right_index=True,how = \"left\")\n",
    "\n",
    "\n",
    "### Model Training\n",
    "# Split data into train and test\n",
    "train,test = train_test_split(telcom, test_size = .25 ,random_state = 42)\n",
    "    \n",
    "# Separate dependent and independent variables and exclude ID column\n",
    "cols    = [i for i in telcom.columns if i not in Id_col + target_col]\n",
    "train_X = train[cols]\n",
    "train_Y = train[target_col]\n",
    "test_X  = test[cols]\n",
    "test_Y  = test[target_col]\n",
    "\n",
    "# Write a function for generic model training    \n",
    "def telecom_churn_prediction(algorithm,training_x,testing_x,\n",
    "                             training_y,testing_y,cols,cf):\n",
    "    '''\n",
    "    Function signature\n",
    "    algorithm     - algorithm used \n",
    "    training_x    - predictor variables dataframe (training)\n",
    "    testing_x     - predictor variables dataframe (testing)\n",
    "    training_y    - target variable (training)\n",
    "    training_y    - target variable (testing)\n",
    "    cols          - features\n",
    "    cf - [\"coefficients\",\"features\"] (cooefficients for logistic regression, features for tree based models)\n",
    "    '''\n",
    "    \n",
    "    # Fit the model\n",
    "    algorithm.fit(training_x,training_y)\n",
    "    predictions   = algorithm.predict(testing_x)\n",
    "    probabilities = algorithm.predict_proba(testing_x)\n",
    "    \n",
    "    # Store coefficients\n",
    "    if   cf == \"coefficients\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n",
    "    elif cf == \"features\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n",
    "        \n",
    "    column_df     = pd.DataFrame(cols)\n",
    "    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                              right_index= True, how = \"left\"))\n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    class_report = classification_report(testing_y,predictions)\n",
    "    class_metrics = precision_recall_fscore_support(testing_y, predictions, beta=1.0, average=\"binary\", pos_label=0)\n",
    "    accuracy = accuracy_score(testing_y,predictions)\n",
    "    conf_matrix = confusion_matrix(testing_y,predictions)\n",
    "    model_roc_auc = roc_auc_score(testing_y,predictions)\n",
    "    \n",
    "    precision = class_metrics[0]\n",
    "    recall = class_metrics[1]\n",
    "    fscore = class_metrics[2]\n",
    "    \n",
    "    # Log and print model evaluation information\n",
    "    print (algorithm)\n",
    "    \n",
    "    print (\"\\n Classification report : \\n\", class_report)\n",
    "    \n",
    "    print (\"Accuracy Score : \", accuracy)\n",
    "    run.log(\"Accuracy\", np.float(accuracy))    \n",
    "    \n",
    "    print (\"Area Under Curve : \",model_roc_auc,\"\\n\")\n",
    "    run.log(\"AUC\", np.float(model_roc_auc))\n",
    "    \n",
    "    print (\"Precision : \",precision,\"\\n\")\n",
    "    run.log(\"Precision\", np.float(precision))\n",
    "    \n",
    "    print (\"Recall : \",recall,\"\\n\")\n",
    "    run.log(\"Recall\", np.float(recall))\n",
    "    \n",
    "    print (\"F Score : \",fscore,\"\\n\")\n",
    "    run.log(\"F Score\", np.float(fscore))\n",
    "    \n",
    "    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    class_names = [\"Churn\", \"Non Churn\"]\n",
    "    \n",
    "    trace1 = plot_confusion_matrix(algorithm, testing_x, testing_y,\n",
    "                                   display_labels=class_names,\n",
    "                                   cmap=\"inferno\",\n",
    "                                   normalize=\"true\")\n",
    "    \n",
    "    trace1.ax_.set_title(\"Normalized Confusion Matrix\")\n",
    "    trace1.ax_.grid(False)\n",
    "    run.log_image(\"Normalized Confusion Matrix\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot roc curve\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % model_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    run.log_image(\"Receiver Operating Characteristic\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    x = coef_sumry[\"features\"]\n",
    "    y = coef_sumry[\"coefficients\"]\n",
    "\n",
    "    mask1 = y >= 0\n",
    "    mask2 = y < 0\n",
    "\n",
    "    objects = coef_sumry[\"features\"]\n",
    "    x_pos = np.arange(len(coef_sumry[\"features\"]))\n",
    "\n",
    "    plt.bar(x[mask1], y[mask1], color = 'green')\n",
    "    plt.bar(x[mask2], y[mask2], color = 'red')\n",
    "    plt.xticks(x_pos, x, rotation='vertical')\n",
    "    plt.yticks(rotation='horizontal')\n",
    "    plt.ylabel('Feature Coefficient')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    run.log_image(\"Feature Importance\", plot=plt)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    '''# Plot confusion matrix\n",
    "    trace1 = go.Heatmap(z = conf_matrix ,\n",
    "                        x = [\"Not churn\",\"Churn\"],\n",
    "                        y = [\"Not churn\",\"Churn\"],\n",
    "                        showscale  = False,colorscale = \"Picnic\",\n",
    "                        name = \"matrix\")\n",
    "    \n",
    "    # Plot roc curve\n",
    "    trace2 = go.Scatter(x = fpr,y = tpr,\n",
    "                        name = \"Roc : \" + str(model_roc_auc),\n",
    "                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n",
    "    trace3 = go.Scatter(x = [0,1],y=[0,1],\n",
    "                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n",
    "                        dash = 'dot'))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n",
    "                    name = \"coefficients\",\n",
    "                    marker = dict(color = coef_sumry[\"coefficients\"],\n",
    "                                  colorscale = \"Picnic\",\n",
    "                                  line = dict(width = .6,color = \"black\")))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = sp.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver operating characteristic',\n",
    "                                            'Feature Importances'))\n",
    "    \n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,2)\n",
    "    fig.append_trace(trace3,1,2)\n",
    "    fig.append_trace(trace4,2,1)\n",
    "    \n",
    "    # Plot layout\n",
    "    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n",
    "                         autosize = False,height = 900,width = 800,\n",
    "                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         margin = dict(b = 195))\n",
    "    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n",
    "    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n",
    "    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n",
    "                                        tickangle = 90))\n",
    "    \n",
    "    image_path = \"outputs/log_reg_graphs.png\"\n",
    "    fig.write_image(image_path)\n",
    "    \n",
    "    # Upload the file explicitly into artifacts \n",
    "    run.upload_file(name = image_path, path_or_stream = image_path)'''\n",
    "    \n",
    "    return algorithm\n",
    "        \n",
    "# Build logistic regression model\n",
    "\n",
    "# Expose regularization hyperparameter as script argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "run.log('Regularization Hyperparameter Lambda', np.float(reg)) # this is retrieved as script argument\n",
    "\n",
    "logit = LogisticRegression(C=1/reg, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "logit_fitted = telecom_churn_prediction(logit,train_X,test_X,train_Y,test_Y,\n",
    "                         cols,\"coefficients\")\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "joblib.dump(value=logit_fitted, filename='outputs/customer_churn_log_reg_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
